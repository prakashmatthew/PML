<html>

<head>
<title>Title</title>
</head>

<body>


<p> I started the analysis by using brute force not trying to understand the data first as it seemed too big. I tried different rpart and random forest models on the entire data,after <b>partitioning the train data set</b> which took hours to train on my dual core i5 laptop.<br> I then tried to make models for each <b>user_name</b> splitting the train data as per <b>user_name</b>. All these models failed to (1) train fast (2)predict for all test variables [always had missing values in the prediction, I still don't understand why] <br><b><u>Reducing number of Predictors </u></b><br> I started by eliminating predictors in the <b>train data set</b> which were highly correlated with each other and which had no values at all.I assigned a value of <b>0</b> to predictors where they had missing values and tried various randomforest and rpart models. Even these models had the 2 failures as before.<br><br><b><u>Breakthrough</b></u><br>Then it hit me to check the test data set for missing predictors as models based on train data set won't work on the actual test set if they don't even have those variables. So removing the predictors which had missing values in the test data set the number of predictors came down to 59 from 120. On this I made different models and the model using rf gave the best result and predicted all 20 test cases correctly :D<br><br><b><u>Key Codes</b></u><br><b>Cross Validation</b><br>
train9 <- train8[,-59] #train dataset of with 58 predictors<br>
train.label <- as.factor(train8$classe)<br>

cv.3folds <- createMultiFolds(train.label, k = 3, times = 5)<br>
ctrl1 <- trainControl(method = "repeatedcv", number = 3, repeats = 5, index = cv.3folds)<br>

library(doSNOW) #for faster modelling<br>
cl <- makeCluster(3, type = "SOCK")<br>
registerDoSNOW(cl)<br>
m.rf.5 <- train( x = train9, y = train.label, method = "rf", trControl = ctrl1)<br>

stopCluster(cl)<br>

test9 <- test8[,-59]<br>
m.rf.5$finalModel<br>

#Type of random forest: classification<br>
#Number of trees: 500<br>
#No. of variables tried at each split: 30<br>

#OOB estimate of  error rate: 0.07%<br>
#Confusion matrix:<br>
#  A    B    C    D    E  class.error<br>
#A 5580    0    0    0    0 0.0000000000<br>
#B    1 3795    1    0    0 0.0005267316<br>
#C    0    3 3418    1    0 0.0011689071<br>
#D    0    0    4 3210    2 0.0018656716<br>
#E    0    0    0    2 3605 0.0005544774<br><br>
<b>OOB estimate of  error rate: 0.07%</b><br><br>

<b><u>Final Model</b></u><br><br>
m.rf.4 <- train(classe ~ . , method = "rf", data = train8)<br>
p.rf.4 <- predict(m.rf.4, test8)<br>
p.rf.4<br>

#mtry  Accuracy   Kappa  <br>  
#2    0.9905045  0.9879890<br>
#41    0.9991620  0.9989403<br>
#80    0.9983589  0.9979248<br>
#Accuracy was used to select the optimal model using  the largest value.<br>
#The final value used for the model was mtry = 41.<br>
<br>
#m.rf.4 has all the 20 test variables correctly predicted
</p>
</body>
</html>
